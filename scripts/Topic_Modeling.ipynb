{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "**Topic modeling** is a method for ***unsupervised classification*** of such documents, which finds natural groups of items even when we’re not sure what we’re looking for. \n",
        "\n",
        "\n",
        "I introduced the concept of topic modeling and walked through the code for developing your topic model using **Latent Dirichlet Allocation (LDA)** method in the ***python*** using gensim implementation.\n",
        "\n",
        "\n",
        "**Model Implementation Steps:**\n",
        "\n",
        "\n",
        "1. Loading Data\n",
        "2. Data Cleaning\n",
        "3. Phrase Modeling: Bi-grams\n",
        "4. Data Transformation: Corpus and Dictionary\n",
        "5. Base Model: Latent Dirichlet Allocation (LDA) Model \n",
        "6. Hyper-parameter Tuning\n",
        "7. Final model\n",
        "8. Visualize Results"
      ],
      "metadata": {
        "id": "DKcONMcqKnG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "0iLybcsUOQx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iFiPNQSORDY",
        "outputId": "0070f4a0-00ba-4ede-86dd-8c640844a90a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peSQzjkH_r3h"
      },
      "source": [
        "#**1. Loading Data**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42R9mcMO_M6g",
        "outputId": "321b921b-f50c-465f-a0e0-1bf53dcc26d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4999 entries, 0 to 4998\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ArticleID  4999 non-null   int64 \n",
            " 1   Title      4999 non-null   object\n",
            " 2   Abstract   4999 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 117.3+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('Pubmed5k.xlsx')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "rr_mBIGvPR6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My data don't contain on any null values and contains on three columns ArticleID, Title and Abstract. \n",
        "\n",
        "The Objective of task is extracting name topic from Abstract so the other columns isn't important.\n"
      ],
      "metadata": {
        "id": "vm6t-74kPmnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Data Cleaning**"
      ],
      "metadata": {
        "id": "mpONZ-XSQrS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['ArticleID', 'Title'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "11bIC5EzPjp_",
        "outputId": "396e17d9-51b0-403b-afb2-848350e7b95b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Abstract\n",
              "0  Coordination variability (CV) is commonly anal...\n",
              "1  Clinical Scenario: Dynamic knee valgus (DKV) i...\n",
              "2  Various methodologies have been reported to as...\n",
              "3  As outcomes for acute ischemic stroke (AIS) va...\n",
              "4  Because hearing loss in children can result in..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-141d3473-0867-4679-b642-0e147d71d584\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Coordination variability (CV) is commonly anal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clinical Scenario: Dynamic knee valgus (DKV) i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Various methodologies have been reported to as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As outcomes for acute ischemic stroke (AIS) va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Because hearing loss in children can result in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-141d3473-0867-4679-b642-0e147d71d584')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-141d3473-0867-4679-b642-0e147d71d584 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-141d3473-0867-4679-b642-0e147d71d584');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Abstract'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "nUgJRlqVRfBG",
        "outputId": "73818474-745d-45d3-f8d9-43c14be429ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Coordination variability (CV) is commonly analyzed to understand dynamical qualities of human locomotion. The purpose of this study was to develop guidelines for the number of trials required to inform the calculation of a stable mean lower limb CV during overground locomotion. Three-dimensional lower limb kinematics were captured for 10 recreational runners performing 20 trials each of preferred and fixed speed walking and running. Stance phase CV was calculated for 9 segment and joint couplings using a modified vector coding technique. The number of trials required to achieve a CV mean within 10% of 20 strides average was determined for each coupling and individual. The statistical outputs of mode (walking vs running) and speed (preferred vs fixed) were compared when informed by differing numbers of trials. A minimum of 11 trials were required for stable mean stance phase CV. With fewer than 11 trials, CV was underestimated and led to an oversight of significant differences between mode and speed. Future overground locomotion CV research in healthy populations using a vector coding approach should use 11 trials as a standard minimum. Researchers should be aware of the notable consequences of an insufficient number of trials for overall study findings.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "df['Abstract_processed'] = df['Abstract'].map(lambda x: re.sub('[,\\.!?%]', '', x))\n",
        "df['Abstract_processed'] = df['Abstract_processed'].map(lambda x: re.sub(\"\\(.*?\\)\",'',x))\n",
        "\n",
        "\n",
        "# Convert the abstract to lowercase\n",
        "df['Abstract_processed'] = df['Abstract_processed'].map(lambda x: x.lower())\n",
        "\n",
        "df['Abstract_processed'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "zcp4_KF7Q2sO",
        "outputId": "0cb54287-6673-43cb-d394-bb15706b0ec8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'coordination variability  is commonly analyzed to understand dynamical qualities of human locomotion the purpose of this study was to develop guidelines for the number of trials required to inform the calculation of a stable mean lower limb cv during overground locomotion three-dimensional lower limb kinematics were captured for 10 recreational runners performing 20 trials each of preferred and fixed speed walking and running stance phase cv was calculated for 9 segment and joint couplings using a modified vector coding technique the number of trials required to achieve a cv mean within 10 of 20 strides average was determined for each coupling and individual the statistical outputs of mode  and speed  were compared when informed by differing numbers of trials a minimum of 11 trials were required for stable mean stance phase cv with fewer than 11 trials cv was underestimated and led to an oversight of significant differences between mode and speed future overground locomotion cv research in healthy populations using a vector coding approach should use 11 trials as a standard minimum researchers should be aware of the notable consequences of an insufficient number of trials for overall study findings'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize words and further clean-up text**\n",
        "\n",
        "Tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
      ],
      "metadata": {
        "id": "D0rPooUfSHyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def convert_sentences_into_words(sentences):\n",
        "    for text in sentences:\n",
        "        yield(simple_preprocess(str(text), deacc=True))  # I used deacc to remove punctuations\n",
        "\n",
        "data_sentences = df['Abstract_processed'].values.tolist()\n",
        "data_words = list(convert_sentences_into_words(data_sentences))\n",
        "\n",
        "data_words[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD5cMRWmQ21e",
        "outputId": "795d5759-fd92-4a91-85ca-b3723f531a05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['coordination',\n",
              " 'variability',\n",
              " 'is',\n",
              " 'commonly',\n",
              " 'analyzed',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'dynamical',\n",
              " 'qualities',\n",
              " 'of',\n",
              " 'human',\n",
              " 'locomotion',\n",
              " 'the',\n",
              " 'purpose',\n",
              " 'of',\n",
              " 'this',\n",
              " 'study',\n",
              " 'was',\n",
              " 'to',\n",
              " 'develop',\n",
              " 'guidelines',\n",
              " 'for',\n",
              " 'the',\n",
              " 'number',\n",
              " 'of',\n",
              " 'trials',\n",
              " 'required',\n",
              " 'to',\n",
              " 'inform',\n",
              " 'the',\n",
              " 'calculation',\n",
              " 'of',\n",
              " 'stable',\n",
              " 'mean',\n",
              " 'lower',\n",
              " 'limb',\n",
              " 'cv',\n",
              " 'during',\n",
              " 'overground',\n",
              " 'locomotion',\n",
              " 'three',\n",
              " 'dimensional',\n",
              " 'lower',\n",
              " 'limb',\n",
              " 'kinematics',\n",
              " 'were',\n",
              " 'captured',\n",
              " 'for',\n",
              " 'recreational',\n",
              " 'runners',\n",
              " 'performing',\n",
              " 'trials',\n",
              " 'each',\n",
              " 'of',\n",
              " 'preferred',\n",
              " 'and',\n",
              " 'fixed',\n",
              " 'speed',\n",
              " 'walking',\n",
              " 'and',\n",
              " 'running',\n",
              " 'stance',\n",
              " 'phase',\n",
              " 'cv',\n",
              " 'was',\n",
              " 'calculated',\n",
              " 'for',\n",
              " 'segment',\n",
              " 'and',\n",
              " 'joint',\n",
              " 'couplings',\n",
              " 'using',\n",
              " 'modified',\n",
              " 'vector',\n",
              " 'coding',\n",
              " 'technique',\n",
              " 'the',\n",
              " 'number',\n",
              " 'of',\n",
              " 'trials',\n",
              " 'required',\n",
              " 'to',\n",
              " 'achieve',\n",
              " 'cv',\n",
              " 'mean',\n",
              " 'within',\n",
              " 'of',\n",
              " 'strides',\n",
              " 'average',\n",
              " 'was',\n",
              " 'determined',\n",
              " 'for',\n",
              " 'each',\n",
              " 'coupling',\n",
              " 'and',\n",
              " 'individual',\n",
              " 'the',\n",
              " 'statistical',\n",
              " 'outputs',\n",
              " 'of',\n",
              " 'mode',\n",
              " 'and',\n",
              " 'speed',\n",
              " 'were',\n",
              " 'compared',\n",
              " 'when',\n",
              " 'informed',\n",
              " 'by',\n",
              " 'differing',\n",
              " 'numbers',\n",
              " 'of',\n",
              " 'trials',\n",
              " 'minimum',\n",
              " 'of',\n",
              " 'trials',\n",
              " 'were',\n",
              " 'required',\n",
              " 'for',\n",
              " 'stable',\n",
              " 'mean',\n",
              " 'stance',\n",
              " 'phase',\n",
              " 'cv',\n",
              " 'with',\n",
              " 'fewer',\n",
              " 'than',\n",
              " 'trials',\n",
              " 'cv',\n",
              " 'was',\n",
              " 'underestimated',\n",
              " 'and',\n",
              " 'led',\n",
              " 'to',\n",
              " 'an',\n",
              " 'oversight',\n",
              " 'of',\n",
              " 'significant',\n",
              " 'differences',\n",
              " 'between',\n",
              " 'mode',\n",
              " 'and',\n",
              " 'speed',\n",
              " 'future',\n",
              " 'overground',\n",
              " 'locomotion',\n",
              " 'cv',\n",
              " 'research',\n",
              " 'in',\n",
              " 'healthy',\n",
              " 'populations',\n",
              " 'using',\n",
              " 'vector',\n",
              " 'coding',\n",
              " 'approach',\n",
              " 'should',\n",
              " 'use',\n",
              " 'trials',\n",
              " 'as',\n",
              " 'standard',\n",
              " 'minimum',\n",
              " 'researchers',\n",
              " 'should',\n",
              " 'be',\n",
              " 'aware',\n",
              " 'of',\n",
              " 'the',\n",
              " 'notable',\n",
              " 'consequences',\n",
              " 'of',\n",
              " 'an',\n",
              " 'insufficient',\n",
              " 'number',\n",
              " 'of',\n",
              " 'trials',\n",
              " 'for',\n",
              " 'overall',\n",
              " 'study',\n",
              " 'findings']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Phrase Modeling: Bigrams**\n",
        "\n",
        "***Bigrams*** are two words frequently occurring together in the document. \n"
      ],
      "metadata": {
        "id": "MSQ_zItcXK5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "bigram = gensim.models.Phrases(data_words, min_count=10, threshold=50) \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n"
      ],
      "metadata": {
        "id": "ZCJV_UNeQ25F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stopword, Make Bigrams and Lemmatize"
      ],
      "metadata": {
        "id": "yyTLZrCHX5kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGFzO1YeXxvA",
        "outputId": "5c9474c8-ff5d-42a8-ce51-15457f0b8d1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "AnKQJGMCXx-d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "data_lemmatized[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIYJi-eoZwrr",
        "outputId": "33363551-7683-4d96-d6a8-ed7c533d2e81"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['coordination',\n",
              " 'variability',\n",
              " 'commonly',\n",
              " 'analyze',\n",
              " 'understand',\n",
              " 'dynamical',\n",
              " 'quality',\n",
              " 'locomotion',\n",
              " 'purpose',\n",
              " 'study',\n",
              " 'develop',\n",
              " 'guideline',\n",
              " 'number',\n",
              " 'trial',\n",
              " 'require',\n",
              " 'inform',\n",
              " 'calculation',\n",
              " 'stable',\n",
              " 'overground',\n",
              " 'locomotion',\n",
              " 'kinematic',\n",
              " 'capture',\n",
              " 'recreational',\n",
              " 'runner',\n",
              " 'perform',\n",
              " 'trial',\n",
              " 'prefer',\n",
              " 'fix',\n",
              " 'speed',\n",
              " 'walk',\n",
              " 'run',\n",
              " 'stance',\n",
              " 'phase',\n",
              " 'cv',\n",
              " 'calculated',\n",
              " 'segment',\n",
              " 'joint',\n",
              " 'coupling',\n",
              " 'use',\n",
              " 'modify',\n",
              " 'vector',\n",
              " 'coding',\n",
              " 'technique',\n",
              " 'number',\n",
              " 'trial',\n",
              " 'require',\n",
              " 'achieve',\n",
              " 'cv',\n",
              " 'mean',\n",
              " 'stride',\n",
              " 'average',\n",
              " 'determine',\n",
              " 'couple',\n",
              " 'individual',\n",
              " 'statistical',\n",
              " 'outputs',\n",
              " 'mode',\n",
              " 'speed',\n",
              " 'compare',\n",
              " 'inform',\n",
              " 'differ',\n",
              " 'number',\n",
              " 'trial',\n",
              " 'minimum',\n",
              " 'trial',\n",
              " 'require',\n",
              " 'stable',\n",
              " 'mean',\n",
              " 'stance',\n",
              " 'phase',\n",
              " 'few',\n",
              " 'trial',\n",
              " 'underestimate',\n",
              " 'lead',\n",
              " 'oversight',\n",
              " 'mode',\n",
              " 'speed',\n",
              " 'future',\n",
              " 'overground',\n",
              " 'locomotion',\n",
              " 'cv',\n",
              " 'research',\n",
              " 'healthy',\n",
              " 'population',\n",
              " 'use',\n",
              " 'vector',\n",
              " 'code',\n",
              " 'approach',\n",
              " 'use',\n",
              " 'trial',\n",
              " 'standard',\n",
              " 'minimum',\n",
              " 'researcher',\n",
              " 'aware',\n",
              " 'notable',\n",
              " 'consequence',\n",
              " 'insufficient',\n",
              " 'number',\n",
              " 'trial',\n",
              " 'overall',\n",
              " 'study',\n",
              " 'finding']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Data transformation: Corpus and Dictionary**\n",
        "\n",
        "Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model, and it needs two inputs that are the dictionary and the corpus."
      ],
      "metadata": {
        "id": "HnUKv8CDbfEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = gensim.corpora.Dictionary(data_lemmatized)  \n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in data_lemmatized]"
      ],
      "metadata": {
        "id": "yL3nsBNtbd-t"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **5. Base Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "i8RKEOspdHdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word,   num_topics=3, random_state=100,chunksize=100,passes=10)"
      ],
      "metadata": {
        "id": "lriITqL_beZ0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "DejVB5Z1dkaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f6aa6c-0c61-457b-e930-2c5c125b6cc7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.014*\"use\" + 0.008*\"base\" + 0.008*\"model\" + 0.007*\"method\" + 0.005*\"system\" + 0.004*\"result\" + 0.004*\"provide\" + 0.004*\"process\" + 0.004*\"study\" + 0.004*\"time\"'),\n",
              " (1,\n",
              "  '0.012*\"cell\" + 0.008*\"effect\" + 0.008*\"study\" + 0.007*\"protein\" + 0.007*\"increase\" + 0.006*\"show\" + 0.006*\"high\" + 0.006*\"gene\" + 0.006*\"use\" + 0.005*\"level\"'),\n",
              " (2,\n",
              "  '0.018*\"patient\" + 0.016*\"study\" + 0.011*\"use\" + 0.006*\"risk\" + 0.006*\"health\" + 0.006*\"include\" + 0.006*\"high\" + 0.005*\"group\" + 0.005*\"year\" + 0.004*\"treatment\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Coherence Score**\n",
        "\n",
        "\n",
        "***Topic Coherence*** measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. \n",
        "\n",
        "\n",
        "The coherence measures are used in this task that's **C_v**.\n",
        "\n",
        "\n",
        "***C_v measure*** is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
      ],
      "metadata": {
        "id": "1SN2pr3xeQrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQg62H2AePP9",
        "outputId": "9cda0f5c-da2b-4fbf-968b-b3d6556a7357"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.3771509216529963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Hyperparameter tuning**\n",
        "\n",
        "First, we must know what's the difference between model hyperparameters and model parameters ?\n",
        "\n",
        "\n",
        "\n",
        "*  ***Model hyperparameters*** can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training, such number of topics, alpha and beta.\n",
        "\n",
        "\n",
        "*   ***Model parameters*** can be thought of as what the model learns during training, such as the weights for each word in a text.\n",
        "\n",
        "\n",
        "we have the coherence score for the LDA model, perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
        "\n",
        "*   Number of Topics    \n",
        "*   Hyperparameter alpha (Document Density)\n",
        "*   Hyperparameter beta  (Word Density)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DYKrpLSXiOp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(corpus, dictionary,k,a, b):\n",
        "    \n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "    \n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    \n",
        "    return coherence_model_lda.get_coherence()"
      ],
      "metadata": {
        "id": "fM-61XNEkp15"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "#Number of Topics\n",
        "topics_range = range(3, 11, 1)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.5))\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.5))\n",
        "\n",
        "\n",
        "model_results = {\n",
        "                 'Num_topics':[],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)))\n",
        "\n",
        "\n",
        "for k in topics_range:\n",
        "  for a in alpha:\n",
        "    for b in beta:\n",
        "\n",
        "      cv = compute_coherence_values(corpus, id2word,k=k, a=a, b=b)\n",
        "      model_results['Num_topics'].append(k)\n",
        "      model_results['Alpha'].append(a)\n",
        "      model_results['Beta'].append(b)\n",
        "      model_results['Coherence'].append(cv)\n",
        "      pbar.update(1)\n",
        "                    \n",
        "                 \n",
        "pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
        "pbar.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIFWfZKPlUrI",
        "outputId": "66896b51-0321-403c-d109-5b73632dcdd3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n",
            "100%|██████████| 32/32 [40:39<00:00, 76.22s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Final model**"
      ],
      "metadata": {
        "id": "6IxAA50W0vtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=9, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.51,\n",
        "                                           eta=0.51)"
      ],
      "metadata": {
        "id": "CqgRIxgulU8f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.print_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Wj0kEs1ahp",
        "outputId": "a6b8353f-b1ad-4559-c665-a22982a6dd16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.005*\"channel\" + 0.005*\"formulation\" + 0.004*\"oxide\" + 0.004*\"iron\" + 0.004*\"nanoparticle\" + 0.003*\"charge\" + 0.003*\"compression\" + 0.003*\"release\" + 0.002*\"ingredient\" + 0.002*\"polyphenol\"'),\n",
              " (1,\n",
              "  '0.020*\"cell\" + 0.012*\"protein\" + 0.008*\"effect\" + 0.006*\"expression\" + 0.006*\"increase\" + 0.006*\"show\" + 0.006*\"activity\" + 0.006*\"induce\" + 0.006*\"study\" + 0.005*\"mechanism\"'),\n",
              " (2,\n",
              "  '0.005*\"smoking\" + 0.005*\"smoker\" + 0.003*\"smoke\" + 0.003*\"tobacco\" + 0.002*\"cessation\" + 0.002*\"cigarette\" + 0.001*\"nicotine\" + 0.001*\"exudative\" + 0.001*\"fry\" + 0.001*\"apoa\"'),\n",
              " (3,\n",
              "  '0.029*\"patient\" + 0.019*\"study\" + 0.013*\"use\" + 0.010*\"group\" + 0.009*\"high\" + 0.009*\"risk\" + 0.008*\"treatment\" + 0.007*\"include\" + 0.007*\"year\" + 0.007*\"age\"'),\n",
              " (4,\n",
              "  '0.015*\"use\" + 0.008*\"model\" + 0.007*\"base\" + 0.007*\"study\" + 0.007*\"method\" + 0.006*\"result\" + 0.005*\"high\" + 0.005*\"provide\" + 0.005*\"specie\" + 0.004*\"different\"'),\n",
              " (5,\n",
              "  '0.012*\"health\" + 0.012*\"study\" + 0.009*\"use\" + 0.006*\"care\" + 0.005*\"intervention\" + 0.005*\"research\" + 0.005*\"experience\" + 0.004*\"participant\" + 0.004*\"impact\" + 0.004*\"conduct\"'),\n",
              " (6,\n",
              "  '0.011*\"case\" + 0.011*\"surgery\" + 0.008*\"patient\" + 0.008*\"surgical\" + 0.008*\"implant\" + 0.006*\"bone\" + 0.006*\"present\" + 0.005*\"perform\" + 0.005*\"complication\" + 0.005*\"follow\"'),\n",
              " (7,\n",
              "  '0.018*\"infection\" + 0.015*\"virus\" + 0.012*\"genetic\" + 0.012*\"gene\" + 0.008*\"sequence\" + 0.008*\"viral\" + 0.007*\"antibody\" + 0.007*\"genome\" + 0.006*\"host\" + 0.006*\"isolate\"'),\n",
              " (8,\n",
              "  '0.005*\"music\" + 0.004*\"facial\" + 0.003*\"pedestrian\" + 0.003*\"vehicle\" + 0.002*\"musical\" + 0.002*\"listen\" + 0.002*\"crosswalk\" + 0.002*\"postural\" + 0.001*\"ssp\" + 0.001*\"sit\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=final_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKeK9hIJ1uDj",
        "outputId": "8d5d7cdf-ffe6-40b4-effc-768a352ac33a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.48941576578317786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8. Visualize Results**\n"
      ],
      "metadata": {
        "id": "yM2ARy-__Pi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LK6Wc7VGXla",
        "outputId": "1ce8048c-c68f-4e60-ddc9-8be2669ac045"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=2f9119a1f7d1d3a539227558e1c43fc0ea9b65ffa880f485d0c4f42bf0c76856\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_prepared = pyLDAvis.gensim_models.prepare(final_model, corpus, id2word)\n",
        "pyLDAvis.save_html(LDAvis_prepared, 'lDAvis.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Fep1cE_v92",
        "outputId": "3a808aeb-6e75-41f5-84e7-20d1c3a7dd18"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Topic Modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}